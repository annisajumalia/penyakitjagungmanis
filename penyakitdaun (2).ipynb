{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.optimizers\n",
        "import tensorflow as tf\n",
        "\n",
        "import cv2\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "KRIXYMUJEufq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_S2sEgd4E-Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/dataset/dataset-jagung/data-excel/PenyakitDaunJagung.csv\")\n",
        "data.head()\n",
        "\n",
        "# /content/drive/MyDrive/dataset/dataset-jagung/data-excel/hawar_daun.xlsx\n",
        "# /content/drive/MyDrive/dataset/dataset-jagung/data-excel/bercak_daun.xlsx\n",
        "# /content/drive/MyDrive/dataset/dataset-jagung/data-excel/Karatan_daun.xlsx"
      ],
      "metadata": {
        "id": "blQM7YOXPGbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Path ke file Excel\n",
        "path_bercak = '/content/drive/MyDrive/dataset/dataset-jagung/data-excel/bercak_daun.xlsx'\n",
        "path_hawar = '/content/drive/MyDrive/dataset/dataset-jagung/data-excel/hawar_daun.xlsx'\n",
        "path_karatan = '/content/drive/MyDrive/dataset/dataset-jagung/data-excel/Karatan_daun.xlsx'\n",
        "\n",
        "# Membaca data dari masing-masing file Excel\n",
        "df_bercak = pd.read_excel(path_bercak)\n",
        "df_hawar = pd.read_excel(path_hawar)\n",
        "df_karatan = pd.read_excel(path_karatan)\n",
        "\n",
        "# Menggabungkan data\n",
        "df_gabungan = pd.concat([df_bercak, df_hawar, df_karatan], ignore_index=True)\n",
        "\n",
        "# Menyimpan data gabungan ke file Excel baru\n",
        "output_path = '/content/drive/MyDrive/dataset/dataset-jagung/data-excel/singleObjectDetection.xlsx'\n",
        "df_gabungan.to_excel(output_path, index=False)\n",
        "\n",
        "print(\"Data berhasil digabungkan dan disimpan di:\", output_path)"
      ],
      "metadata": {
        "id": "ucsBUeelmhGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel('/content/drive/MyDrive/dataset/dataset-jagung/data-excel/singleObjectDetection.xlsx')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "kvzIe0M0nAmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: total data\n",
        "\n",
        "print(data.info())\n",
        "print(data.describe())\n"
      ],
      "metadata": {
        "id": "PzfTV-o-nGGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[:983]\n",
        "len(data)"
      ],
      "metadata": {
        "id": "jqTt7q_UE6De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from PIL import ImageFile\n",
        "import numpy as np\n",
        "\n",
        "# Allow loading of truncated images\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "image_size = 224\n",
        "image_list = []\n",
        "invalid_images = []\n",
        "\n",
        "# Loop through the dataset and load images\n",
        "for i in tqdm(range(data.shape[0])):\n",
        "    try:\n",
        "        img = image.load_img('/content/drive/MyDrive/dataset/dataset-jagung/images/' + data['image'][i], target_size=(image_size, image_size, 3))\n",
        "        img = image.img_to_array(img)\n",
        "        img = img / 255.0\n",
        "        image_list.append(img)\n",
        "    except (OSError, ValueError) as e:\n",
        "        print(f\"Error loading image {data['image'][i]}: {e}\")\n",
        "        invalid_images.append(i)\n",
        "\n",
        "# Remove invalid images from the DataFrame\n",
        "data = data.drop(invalid_images).reset_index(drop=True)\n",
        "\n",
        "# Convert the list of images to a NumPy array\n",
        "X = np.array(image_list)"
      ],
      "metadata": {
        "id": "uJdfRh7oRiGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_list = np.array(image_list)\n",
        "print(image_list)"
      ],
      "metadata": {
        "id": "VVU7oiCPPlsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Categorical Data\n",
        "y = data[['Hawar Daun', 'Bercak Daun', 'Karatan Daun']].values\n",
        "y"
      ],
      "metadata": {
        "id": "4VF-NiHPP0nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_testval, y_train, y_testval = train_test_split(image_list, y, test_size=0.2, random_state=42) #80% Train, 10% Validation, 10% Test\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_testval, y_testval, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "x7uKvfbiP0k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "dT7ieFRB5flo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = InceptionV3(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))"
      ],
      "metadata": {
        "id": "dmrfklsuP0hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tambahkan lapisan tambahan ke model base Anda\n",
        "x = GlobalAveragePooling2D()(model.output)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "output = Dense(3, activation='sigmoid')(x)"
      ],
      "metadata": {
        "id": "rKHMBqH7UuXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Model(inputs=model.input, outputs=output)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "7vmMDXgEUzFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "# model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                    loss='binary_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "# Tambahkan callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n"
      ],
      "metadata": {
        "id": "yN_lyruvUzDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history = model.fit(x_train,\n",
        "#                     y_train,\n",
        "#                     epochs=20,\n",
        "#                     batch_size=64,\n",
        "#                     validation_data=(x_val, y_val)\n",
        "#                     )\n",
        "\n",
        "# Train model\n",
        "history = model.fit(x_train,\n",
        "                          y_train,\n",
        "                          epochs=30,\n",
        "                          batch_size=16,\n",
        "                          validation_data=(x_val, y_val),\n",
        "                          callbacks=[early_stopping, reduce_lr])"
      ],
      "metadata": {
        "id": "qvfKz3jJUy_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: tampilkan # Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eSvTtcHcU9Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Asumsikan x_test dan y_test sudah tersedia\n",
        "\n",
        "# Evaluasi model pada test set\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(int)  # Konversi probabilitas menjadi label kelas\n",
        "\n",
        "# Hitung akurasi\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Buat laporan klasifikasi\n",
        "print(classification_report(y_test, y_pred_classes, target_names=['Hawar Daun', 'Bercak Daun', 'Karatan Daun']))\n",
        "\n",
        "# Buat confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred_classes.argmax(axis=1))\n",
        "\n",
        "# Visualisasi confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Hawar Daun', 'Bercak Daun', 'Karatan Daun'], yticklabels=['Hawar Daun', 'Bercak Daun', 'Karatan Daun'])\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z2_hWxbHc5mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan Model Tflite\n",
        "# Menyimpan model dalam format tflite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Simpan model tflite ke file\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "metadata": {
        "id": "hb7k_9hZZih_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: simpan juga labels.txt nya\n",
        "\n",
        "# Simpan labels.txt\n",
        "with open('labels.txt', 'w') as f:\n",
        "  f.write('Hawar Daun\\n')\n",
        "  f.write('Bercak Daun\\n')\n",
        "  f.write('Karatan Daun\\n')\n",
        "\n",
        "# Code to save the model to your Google Drive (optional)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp model.tflite /content/drive/MyDrive/\n",
        "!cp labels.txt /content/drive/MyDrive/\n"
      ],
      "metadata": {
        "id": "8bF2qzWJTyK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# Load dan preprocess gambar\n",
        "img_path = '/content/drive/MyDrive/dataset/dataset-jagung/hawar/hawar-10.jpg'\n",
        "img = image.load_img(img_path, target_size=(image_size, image_size))\n",
        "img = image.img_to_array(img)\n",
        "img = img / 255.0\n",
        "\n",
        "# Prediksi\n",
        "proba = model.predict(img.reshape(1, image_size, image_size, 3))\n",
        "classes = np.array(['Hawar Daun', 'Bercak Daun', 'Karatan Daun'])  # Gantilah sesuai dengan kelas Anda\n",
        "\n",
        "# Gabungkan kelas dan probabilitas ke dalam satu list\n",
        "class_proba = list(zip(classes, proba[0]))\n",
        "\n",
        "# Urutkan berdasarkan probabilitas (dari tertinggi ke terendah)\n",
        "class_proba_sorted = sorted(class_proba, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Tampilkan hasil peringkat\n",
        "print(\"Hasil Prediksi (Diurutkan berdasarkan Probabilitas):\")\n",
        "for i, (class_name, probability) in enumerate(class_proba_sorted):\n",
        "    print(f\"{i+1}. {class_name}: {probability*100:.2f}%\")\n",
        "\n",
        "# Tampilkan gambar\n",
        "plt.imshow(img)\n",
        "plt.axis('off')  # Sembunyikan sumbu\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aDI9c4s3aBwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Muat model TFLite\n",
        "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Dapatkan input dan output details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Path dataset dan ambil daftar file gambar dari semua subfolder\n",
        "dataset_path = '/content/drive/MyDrive/dataset/dataset-jagung/'\n",
        "image_files = []\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.jpg'):\n",
        "            image_files.append(os.path.join(root, file))\n",
        "\n",
        "# Pilih 10 gambar secara acak\n",
        "sample_files = random.sample(image_files, 10)\n",
        "\n",
        "# Fungsi untuk memproses gambar dan melakukan prediksi\n",
        "def predict_image(img_path):\n",
        "    # Load and preprocess the image\n",
        "    img = image.load_img(img_path, target_size=(image_size, image_size))\n",
        "    img = image.img_to_array(img)\n",
        "    img = img / 255.0\n",
        "\n",
        "    # Predict\n",
        "    interpreter.set_tensor(input_details[0]['index'], img.reshape(1, image_size, image_size, 3).astype(np.float32))\n",
        "    interpreter.invoke()\n",
        "    proba = interpreter.get_tensor(output_details[0]['index'])[0]\n",
        "\n",
        "    # Apply threshold\n",
        "    threshold = 0.5\n",
        "    classes = np.array(data.columns[2:])\n",
        "    top_classes = [classes[i] for i in range(len(proba)) if proba[i] > threshold]\n",
        "    top_proba = [proba[i] for i in range(len(proba)) if proba[i] > threshold]\n",
        "\n",
        "    return img, top_classes, top_proba\n",
        "\n",
        "# Setup plot\n",
        "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Predict and plot images\n",
        "for ax, file_name in zip(axes, sample_files):\n",
        "    img_path = file_name\n",
        "    img, top_classes, top_proba = predict_image(img_path)\n",
        "\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')\n",
        "    result_str = ', '.join([f\"{cls} ({p:.2f})\" for cls, p in zip(top_classes, top_proba)])\n",
        "    ax.set_title(result_str, fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rGPs_t67ZRJe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}